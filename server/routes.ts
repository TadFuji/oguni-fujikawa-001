import type { Express } from "express";
import { createServer, type Server } from "http";
import { storage } from "./storage";
import { insertConversationSchema, difyRequestSchema, difyResponseSchema } from "@shared/schema";
import { z } from "zod";
import OpenAI from "openai";

// Initialize OpenAI client globally for reuse and better performance
const openai = new OpenAI({ 
  apiKey: process.env.OPENAI_API_KEY,
  timeout: 10000 // 10 second timeout for faster failure detection
});

// Simple in-memory cache for TTS responses
const ttsCache = new Map<string, Buffer>();
const MAX_CACHE_SIZE = 100;

// Helper function to manage cache
function addToCache(key: string, buffer: Buffer) {
  if (ttsCache.size >= MAX_CACHE_SIZE) {
    const firstKey = ttsCache.keys().next().value;
    if (firstKey) {
      ttsCache.delete(firstKey);
    }
  }
  ttsCache.set(key, buffer);
}

export async function registerRoutes(app: Express): Promise<Server> {
  
  // Get conversation history
  app.get("/api/conversations", async (req, res) => {
    try {
      const conversations = await storage.getConversations();
      res.json(conversations);
    } catch (error) {
      res.status(500).json({ message: "ä¼šè©±å±¥æ­´ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ" });
    }
  });

  // OpenAI Text-to-Speech API endpoint
  app.post("/api/tts", async (req, res) => {
    try {
      const { text } = req.body;

      if (!text) {
        return res.status(400).json({ message: "ãƒ†ã‚­ã‚¹ãƒˆãŒå¿…è¦ã§ã™" });
      }

      // Processing TTS request with OpenAI - optimized for speed

      if (!process.env.OPENAI_API_KEY) {
        console.log('ğŸ”Š OpenAI API Keyæœªè¨­å®šã€Web Speech APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯');
        return res.status(404).json({ message: "OpenAI TTSåˆ©ç”¨ä¸å¯ã€Web Speech APIã‚’ä½¿ç”¨" });
      }

      // Simple text processing for speed
      const optimizedText = text.trim();

      // the newest OpenAI model is "gpt-4o" which was released May 13, 2024. do not change this unless explicitly requested by the user
      // Generate speech using OpenAI TTS - optimized for speed
      const mp3 = await openai.audio.speech.create({
        model: "tts-1", // Standard model for faster generation (vs tts-1-hd)
        voice: "alloy", // Balanced, neutral voice suitable for children
        input: optimizedText,
        speed: 1.0, // Normal speaking rate for clearer delivery
        response_format: "mp3"
      });

      // Direct response for maximum speed
      const audioBuffer = Buffer.from(await mp3.arrayBuffer());
      
      res.set({
        'Content-Type': 'audio/mpeg',
        'Content-Length': audioBuffer.length,
      });
      
      res.send(audioBuffer);

    } catch (error) {
      console.error('ğŸ”Š OpenAI TTS ã‚¨ãƒ©ãƒ¼:', error);
      // Return 404 to trigger fallback to Web Speech API
      res.status(404).json({ message: "OpenAI TTSåˆ©ç”¨ä¸å¯ã€Web Speech APIã‚’ä½¿ç”¨" });
    }
  });

  // Send message to Dify and get response
  app.post("/api/chat", async (req, res) => {
    try {
      const { userMessage } = req.body;
      
      if (!userMessage || typeof userMessage !== 'string') {
        return res.status(400).json({ message: "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™" });
      }

      // Get Dify API credentials from environment
      const difyApiKey = process.env.DIFY_API_KEY || process.env.VITE_DIFY_API_KEY;
      
      if (!difyApiKey) {
        return res.status(500).json({ 
          message: "Dify APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚" 
        });
      }

      // Use fixed Dify API endpoint
      const difyApiUrl = "https://api.dify.ai/v1/chat-messages";

      // Prepare Dify API request with consistent user ID
      const difyRequest = {
        inputs: {},
        query: userMessage,
        response_mode: "blocking" as const,
        user: "kid-user-consistent", // Use consistent user ID for better context
      };



      // Use correct API key for Dify
      const correctApiKey = "app-oHgFPuW6h2qOSx49HPjEfbMS";

      // Call Dify API with simpler, faster configuration
      const difyResponse = await fetch(difyApiUrl, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${correctApiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(difyRequest),
      });

      if (!difyResponse.ok) {
        throw new Error(`Dify API error: ${difyResponse.status} ${difyResponse.statusText}`);
      }

      const difyData = await difyResponse.json();
      
      let robotResponse = difyData.answer || "ã€é‡è¦ã€‘Difyã‹ã‚‰è¿”ç­”ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚";
      
      // Simple text cleanup for faster processing
      robotResponse = robotResponse
        .replace(/^Dify ã‹ã‚‰ã®è§£ç­”ã§ã™\n=========\n/, '')
        .replace(/^Dify ã‹ã‚‰ã®è§£ç­”ã§ã™\n/, '')
        .replace(/^=========\n/, '')
        .trim();

      // Calculate metrics for parent dashboard
      const messageLength = userMessage.length;
      const responseLength = robotResponse.length;
      const sessionId = (req as any).sessionID || 'anonymous';
      const duration = Math.max(2, Math.ceil(robotResponse.length / 10)); // Estimate based on response length

      // Save conversation to storage with metrics and raw Dify response
      const conversation = await storage.createConversation({
        userMessage,
        robotResponse,
        rawDifyResponse: difyData.answer, // Store the raw Dify response
        duration,
        messageLength,
        responseLength,
        sessionId,
      });

      res.json({
        conversation,
        robotResponse,
      });

    } catch (error) {
      console.error('ğŸš¨ Chat API error:', error);
      if (error instanceof Error) {
        console.error('ğŸš¨ Error stack:', error.stack);
      }
      const difyApiKey = process.env.DIFY_API_KEY || process.env.VITE_DIFY_API_KEY;
      const difyApiUrl = "https://api.dify.ai/v1/chat-messages";
      console.error('ğŸš¨ Dify API Key exists:', !!difyApiKey);
      console.error('ğŸš¨ Dify App URL:', difyApiUrl);
      
      // Return a fallback response that would be obvious if it appears
      res.status(500).json({ 
        message: "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ä½•ã‹å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚",
        robotResponse: "ã€ã‚¨ãƒ©ãƒ¼ã€‘Difyæ¥ç¶šã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
      });
    }
  });

  // Parent dashboard API routes
  app.get("/api/parent/stats", async (req, res) => {
    try {
      const stats = await storage.getConversationStats();
      res.json(stats);
    } catch (error) {
      console.error("Error fetching stats:", error);
      res.status(500).json({ message: "çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ" });
    }
  });

  app.get("/api/parent/conversations", async (req, res) => {
    try {
      const { startDate, endDate } = req.query;
      let conversations;
      
      if (startDate && endDate) {
        conversations = await storage.getConversationsByDateRange(
          new Date(startDate as string),
          new Date(endDate as string)
        );
      } else {
        conversations = await storage.getConversations();
      }
      
      res.json(conversations);
    } catch (error) {
      console.error("Error fetching conversations:", error);
      res.status(500).json({ message: "ä¼šè©±å±¥æ­´ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ" });
    }
  });

  const httpServer = createServer(app);
  return httpServer;
}
